{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Citation Classification\n",
    "Ka Yu Lau (Gary) last update 25 Jun 2019\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Information overload is a problem in the legal field. Legal professional has spent hours in reading documents. Effort has been made to summarise legal documents in the past two decades. However, due to longer average length, the style of writing and the convoluted and philosophical nature of legalese, legal documents were seen to be more difficult to classify compare to scientific article and news (**Hachey and Grover** )\n",
    "\n",
    "In 2010, UNSW researchers, Galgani and Hoffmann, create a system called 'LEXA'(Legal tEXt Analyzer). The system aimed to reduce legal professionals' information overload by summarizing information in case reports. One of the appoarches LEXA used to acquire knowledge from the documents is through citation classification. \n",
    "\n",
    "Based on *Stare Decisis* in common law, judgement is based on previous cases and decisions on how the law was interpreted. Past decisions has a binding effect on following decisons. Court decisions or cases can be instructive as they introduce a new principle or rule, modify or interpret an existing principle or rule, or settle a question upon which the law is doubtful. Galgani and Hoffmann suggested: by classiying legal citations in the case document, we extract a important information about a legal document which helps the summarisation of legal document.\n",
    "\n",
    "This paper aim to extend the prior work by Galigani and Hoffmann with LEXA,  which used Rule-based method, to other machine learning based method and convolutional neural network.\n",
    "\n",
    "## Related Work\n",
    "\n",
    "**Legal Citation Classification**\n",
    "\n",
    "As mentioned in Galgani and Hoffmann's paper, LEXA's classification rules are created under the monitor of a domain expert and based on regular experssion in the text. Expert made corrections to the rule when error occurs in building their knowledge base. This incremental refinements method is called Ripple-Down Rule (RDR) (Compton and Jansen,1990).  Although domain expert have not involved in our project, we adopted their approach in measuring the performance of the model such as precision, recall and F1-measure and used their result as a benchmark of our models.\n",
    "\n",
    "## Goal of this Project\n",
    "\n",
    "The goal of this project is classifying whether the legal citation is accepted or rejected by the court. The definition of citation is as followed:\n",
    "\n",
    "- **Allow** - A principle of law articulated in the primary case is applied to a new set of facts by the court in the subsequent case.\n",
    "- **Follow** - The annotation is similar to applied but is used in circumstances where the facts in the primary case resemble reasonably closely the facts in the subsequent consideration case\n",
    "- **Distinguished** - The court in the subsequent case holds that the legal principles articulated by the primary case (usually otherwise persuasive or binding authority) do not apply because of some difference between the two cases in fact or law\n",
    "\n",
    "To simplify our classification problem and to be compared to Galgaini's previous work, we have combined the groups into \"**Follow/Allow**\"(FA) or \"**Distinguished**\"(D).\n",
    "\n",
    "## Data\n",
    "\n",
    "**Sample of the dataset**\n",
    "\n",
    "This [dataset] contains Australian legal cases from the Federal Court of Australia (FCA). The data included 3890 cases from the year 2006,2007,2008 and 2009. Citation classes are indicated in the document, and indicate the type of treatment given to the cases cited by the present case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>followed</td>\n",
       "      <td>13 It was not suggested in this proceeding tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>followed</td>\n",
       "      <td>15 Strictly speaking, a cheque, even a bank ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>followed</td>\n",
       "      <td>16 None of this is to suggest that the Deputy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>followed</td>\n",
       "      <td>16 None of this is to suggest that the Deputy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>followed</td>\n",
       "      <td>22 The true position is that the applicant's s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  followed  13 It was not suggested in this proceeding tha...\n",
       "1  followed  15 Strictly speaking, a cheque, even a bank ch...\n",
       "2  followed  16 None of this is to suggest that the Deputy ...\n",
       "3  followed  16 None of this is to suggest that the Deputy ...\n",
       "4  followed  22 The true position is that the applicant's s..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>applied</td>\n",
       "      <td>The recent decision of the High Court in Austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>followed</td>\n",
       "      <td>In Hexal Australia Pty Ltd v Roche Therapeutic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>followed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>followed</td>\n",
       "      <td>The material facts in Purchas, in the matter o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>followed</td>\n",
       "      <td>The material facts in Purchas, in the matter o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   applied  The recent decision of the High Court in Austr...\n",
       "1  followed  In Hexal Australia Pty Ltd v Roche Therapeutic...\n",
       "2  followed                                               None\n",
       "3  followed  The material facts in Purchas, in the matter o...\n",
       "4  followed  The material facts in Purchas, in the matter o..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label     0\n",
      "text     10\n",
      "dtype: int64\n",
      "label    0\n",
      "text     9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum())\n",
    "train = train.dropna()\n",
    "print(test.isnull().sum())\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There 10 missing text in the train set and 9 missing text in test set. This are observation are record are ignored as we cannot extract any features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before transform: ['followed' 'applied' 'distinguished']\n",
      "after transform: [1 0]\n",
      "before transform: ['applied' 'followed' 'distinguished']\n",
      "after transform: [1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "def relabel(df):\n",
    "    '''\n",
    "    relabel follow/applied in to one group and distinguished the other\n",
    "    return: the new dataframe\n",
    "    '''\n",
    "    print('before transform:', df.label.unique())\n",
    "    df.label = df.label.replace('followed',1)\n",
    "    df.label = df.label.replace('applied',1)\n",
    "    df.label = df.label.replace('distinguished',0)\n",
    "    print('after transform:', df.label.unique())\n",
    "    df.label.value_counts()\n",
    "    return df\n",
    "train = relabel(train)\n",
    "test = relabel(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Citation Label: 1\n",
      "Description in Case Report: 13 It was not suggested in this proceeding that, so far as a Commonwealth revenue debt is concerned, there is any express statutory provision which alters the usual position which prevails as between creditor and debtor when a cheque is given in payment of a debt. That usual position was expressed by Mason CJ, Brennan, Deane, Dawson and Toohey JJ in National Australia Bank v KDS Construction Services Pty Ltd [1987] HCA 65 ; (1987) 163 CLR 668 , at 676 to be as follows:\n",
      " Generally speaking, when a cheque is given in payment of a debt, it operates as a conditional payment. The payment is subject to a condition that the cheque be paid on presentation. If it is dishonoured the debt revives. Although it is sometimes said that the remedy for the primary debt is suspended, the suspension is no more than a consequence of the conditional nature of the payment: Tilley v Official Receiver in Bankruptcy [1960] HCA 86 ; (1960) 103 CLR 529 , at pp 532-533, 535-536, 537. The condition is a condition subsequent so that, if the cheque is met, it ranks as an actual payment from the time it was given. Subject to non-fulfilment of the condition subsequent, the payment is complete at the time when the cheque is accepted by the creditor: Thomson v Moyse (1961) AC 967 , at p 1004.\n",
      "\n",
      "14 It was submitted for the Deputy Commissioner that, in the events which had transpired in this case, that usual position had been altered by the signification by the Deputy Commissioner that he would agree to the dismissal of the winding up application once the proceeds of the cheque had been cleared. It seems to me though that the evidence is more consistent with the Deputy Commissioner's adopting a position which was in accordance with the general position when a cheque is given in payment of a debt. It has been accepted subject to a condition that the cheque would be paid on presentation. Subject to the fulfilment of that condition, payment of the then outstanding amount of the debt due to the Commonwealth and payable to the Commissioner was complete upon the acceptance by the Deputy Commissioner on 10 November 2008 of the cheque then proffered on behalf of Ganter. In this regard, the fact that the proffered cheque was a bank cheque may well have provided a degree of comfort to the Deputy Commissioner in deciding to accept it, but the position would have been the same in law so far as the relationship of debtor and creditor was concerned had the cheque concerned been one where the drawer was other than a bank.\n",
      "Training Data Shape: (3950, 2)\n",
      "Testing Data Shape: (1427, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Case Citation Label:', train['label'].iloc[0])\n",
    "print('Description in Case Report:', train['text'].iloc[0])\n",
    "print('Training Data Shape:', train.shape)\n",
    "print('Testing Data Shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAEWCAYAAACHePXKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVKHGPySJHWIwS9JUocY/JIkdcj/B4UuNt7Cb72LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a218d4128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.barplot(x = train['label'].value_counts().index, y=train['label'].value_counts())\n",
    "plt.ylabel('count');plt.xlabel('class')\n",
    "plt.xticks([0,1],('D','FA'))\n",
    "plt.title('Distribution of class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the barplot, the data is severely inbalance. As this will cause prediction problem to us.\n",
    "We will ovrersample it in the later part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "For feature extraction in conventional machine learning, we used the stop words list from spacy to remove the common words in the text. Then the text will be breakdown into tokens. As we would like to experiment the effect of different features, we first calculate tf-idf, count and occurence for bag of words. Then we expand that to 2-gram, 3-gram and mix of unigram, 2gram and 3 gram. \n",
    "\n",
    "Due to the length of the text, it is possible to have a word matrix with features more than the observation. To avoid the curse of dimensionality, we limited our word vector to only having 1000 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utility import feature_extraction\n",
    "n_features = 1000\n",
    "# unigram\n",
    "count,occur,tfidf = feature_extraction(stopwords, (1,1), n_features)\n",
    "# 2gram\n",
    "count_2,occur_2,tfidf_2 = feature_extraction(stopwords, (2,2), n_features)\n",
    "# 3gram\n",
    "count_3,occur_3,tfidf_3 = feature_extraction(stopwords, (3,3), n_features)\n",
    "# mixed 1,2,3 grams\n",
    "count_123,occur_123,tfidf_123 = feature_extraction(stopwords, (1,3), n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling\n",
    "As discussed earlier, the data set is severely imbalance. In order to increase the weight of the minority class, we applied naive random-oversample to oversample the minority class. Based on our grid search result, we oversampled the minority 30% of the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampleling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "**Multinomial Naive Bayes**\n",
    "\n",
    "As mentioned, multinomial naive bayes is used by Galgani. Assuming different class citation summary uses different vocabulary,  multinomial naive bayes applied bayes theorem to classify the document. It takes the probability of token occured in the document(prior distribution) to estimate the probability of which class the citation belongs to (posterior distirbution).\n",
    "\n",
    "**Decsion Tree**\n",
    "\n",
    "LEXA used decision rule based on regular expression to spit the data into two classes. Decsion tree used a similar approach by spitting nodes based on the statistic of  a feature. As some words can appears a lot while some do not, the frequency of token can be very different. Since decision tree is insensitive to extreme values,  it is suitable for this senario.\n",
    "\n",
    "**Logisitic Regression**\n",
    "\n",
    "Treating each word as a predictor in a logistic regression model, logistic regression estimate how much the occurence of certain word increases the chance of the document being one of the group.\n",
    "\n",
    "**Support Vector Machine**\n",
    "\n",
    "Consider each of the word as a dimention, support vector machine tries find a multi-dimensional plane that can seperate the two classes. This assumption certain word can identify class is similar to picking up the conditions in the text that LEXA used.\n",
    "\n",
    "**K-nearest Neighbours**\n",
    "\n",
    "We assumed if two documents used similar words are more likely to belong to the same class. K-nearest Neighbour classified classify based on  which class of the closest K observations of the new observaton belongs to. \n",
    "\n",
    "**Essemble Method: voting**\n",
    "\n",
    "We founded the models we create from the basic technique tends to overfit and produce low prediction for the minority. We tried essemble methods  that combine the results of weaker models to provide a stronger model. The technique we used is called voting. Each of the model provides a result (vote) which class the citation belongs to. The class get the most vote wins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree,linear_model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "mnb = MultinomialNB(alpha=0.7)\n",
    "dtc = tree.DecisionTreeClassifier(max_depth=12)\n",
    "lm = linear_model.LogisticRegression(solver='lbfgs')\n",
    "knn = KNeighborsClassifier(n_neighbors=27)\n",
    "svm  = SVC(gamma='auto')\n",
    "eclf = VotingClassifier(estimators=[('dt', dtc), ('knn', knn), ('svc', svm),('log',lm),('mnb',mnb)], voting='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_precision_D': 0.2605633802816901,\n",
       " '2_recall_D': 0.4625,\n",
       " '3_F1_D': 0.33333333333333337,\n",
       " '4_precision_FA': 0.9247594050743657,\n",
       " '5_recall_FA': 0.8342541436464088,\n",
       " '6_F1_FA': 0.8771784232365145,\n",
       " '7_accuracy': 0.7925718290119131}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_utility import nlp_pipeline, model_score\n",
    "predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, occur, mnb,ros)\n",
    "baseline_score = model_score(test.label.ravel(),predicted)\n",
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use Galgani's result as our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result from Galgani's paper in 2010\n",
    "LEXA_train = {'1_precision_D': 0.784,'2_recall_D': 0.417,'3_F1_D': 0.545,\n",
    "              '4_precision_FA': 0.928, '5_recall_FA': 0.985, '6_F1_FA': 0.955,'7_accuracy': 0.919}\n",
    "LEXA_test = {'1_precision_D': 0.5,'2_recall_D': 0.263,'3_F1_D': 0.344,\n",
    "              '4_precision_FA': 0.913, '5_recall_FA': 0.967, '6_F1_FA': 0.939,'7_accuracy': 0.888}\n",
    "Galgani_NB4s_train = {'1_precision_D': 0.824,'2_recall_D': 0.772,'3_F1_D': 0.797,\n",
    "              '4_precision_FA': 0.971, '5_recall_FA': 0.979, '6_F1_FA': 0.975,'7_accuracy': 0.955}\n",
    "Galgani_NB4s_test = {'1_precision_D': 0.493,'2_recall_D': 0.206,'3_F1_D': 0.291,\n",
    "              '4_precision_FA': 0.907, '5_recall_FA': 0.973, '6_F1_FA': 0.939,'7_accuracy': 0.888}\n",
    "G_train = [LEXA_train,Galgani_NB4s_train]\n",
    "G_test = [LEXA_test,Galgani_NB4s_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylau/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/garylau/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtc</th>\n",
       "      <th>mnb</th>\n",
       "      <th>knn</th>\n",
       "      <th>log</th>\n",
       "      <th>svm</th>\n",
       "      <th>voter</th>\n",
       "      <th>LEXA</th>\n",
       "      <th>G_NB4s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_precision_D</th>\n",
       "      <td>0.842553</td>\n",
       "      <td>0.438735</td>\n",
       "      <td>0.541045</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.969419</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_recall_D</th>\n",
       "      <td>0.860870</td>\n",
       "      <td>0.723913</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.917391</td>\n",
       "      <td>0.113043</td>\n",
       "      <td>0.689130</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_F1_D</th>\n",
       "      <td>0.851613</td>\n",
       "      <td>0.546349</td>\n",
       "      <td>0.398352</td>\n",
       "      <td>0.923414</td>\n",
       "      <td>0.202335</td>\n",
       "      <td>0.805591</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_precision_FA</th>\n",
       "      <td>0.981609</td>\n",
       "      <td>0.960201</td>\n",
       "      <td>0.914449</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.895277</td>\n",
       "      <td>0.960530</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_recall_FA</th>\n",
       "      <td>0.978797</td>\n",
       "      <td>0.877937</td>\n",
       "      <td>0.964756</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>0.997135</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_F1_FA</th>\n",
       "      <td>0.980201</td>\n",
       "      <td>0.917228</td>\n",
       "      <td>0.938929</td>\n",
       "      <td>0.989980</td>\n",
       "      <td>0.944490</td>\n",
       "      <td>0.978490</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_accuracy</th>\n",
       "      <td>0.965063</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.889114</td>\n",
       "      <td>0.982278</td>\n",
       "      <td>0.896203</td>\n",
       "      <td>0.961266</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dtc       mnb       knn       log       svm     voter  \\\n",
       "1_precision_D   0.842553  0.438735  0.541045  0.929515  0.962963  0.969419   \n",
       "2_recall_D      0.860870  0.723913  0.315217  0.917391  0.113043  0.689130   \n",
       "3_F1_D          0.851613  0.546349  0.398352  0.923414  0.202335  0.805591   \n",
       "4_precision_FA  0.981609  0.960201  0.914449  0.989130  0.895277  0.960530   \n",
       "5_recall_FA     0.978797  0.877937  0.964756  0.990831  0.999427  0.997135   \n",
       "6_F1_FA         0.980201  0.917228  0.938929  0.989980  0.944490  0.978490   \n",
       "7_accuracy      0.965063  0.860000  0.889114  0.982278  0.896203  0.961266   \n",
       "\n",
       "                 LEXA  G_NB4s  \n",
       "1_precision_D   0.784   0.824  \n",
       "2_recall_D      0.417   0.772  \n",
       "3_F1_D          0.545   0.797  \n",
       "4_precision_FA  0.928   0.971  \n",
       "5_recall_FA     0.985   0.979  \n",
       "6_F1_FA         0.955   0.975  \n",
       "7_accuracy      0.919   0.955  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = [dtc,mnb,knn,lm,svm,eclf]\n",
    "result = []\n",
    "for c in classifier:\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), train.text, occur,c,ros)\n",
    "    score = model_score(train.label,predicted)\n",
    "    result.append(score)\n",
    "classifier_train = pd.DataFrame(result+G_train, index=['dtc','mnb','knn','log','svm','voter','LEXA','G_NB4s'])\n",
    "classifier_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylau/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/garylau/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dtc</th>\n",
       "      <th>mnb</th>\n",
       "      <th>knn</th>\n",
       "      <th>log</th>\n",
       "      <th>svm</th>\n",
       "      <th>voter</th>\n",
       "      <th>LEXA</th>\n",
       "      <th>G_NB4s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_precision_D</th>\n",
       "      <td>0.216346</td>\n",
       "      <td>0.260563</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.299465</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_recall_D</th>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.231250</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_F1_D</th>\n",
       "      <td>0.244565</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.231660</td>\n",
       "      <td>0.322767</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.328889</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_precision_FA</th>\n",
       "      <td>0.905660</td>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.902108</td>\n",
       "      <td>0.916129</td>\n",
       "      <td>0.889670</td>\n",
       "      <td>0.909692</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_recall_FA</th>\n",
       "      <td>0.871350</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.945541</td>\n",
       "      <td>0.896606</td>\n",
       "      <td>0.999211</td>\n",
       "      <td>0.977901</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_F1_FA</th>\n",
       "      <td>0.888174</td>\n",
       "      <td>0.877178</td>\n",
       "      <td>0.923314</td>\n",
       "      <td>0.906262</td>\n",
       "      <td>0.941264</td>\n",
       "      <td>0.942564</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_accuracy</th>\n",
       "      <td>0.805186</td>\n",
       "      <td>0.792572</td>\n",
       "      <td>0.860547</td>\n",
       "      <td>0.835319</td>\n",
       "      <td>0.889278</td>\n",
       "      <td>0.894184</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dtc       mnb       knn       log       svm     voter  \\\n",
       "1_precision_D   0.216346  0.260563  0.303030  0.299465  0.750000  0.569231   \n",
       "2_recall_D      0.281250  0.462500  0.187500  0.350000  0.018750  0.231250   \n",
       "3_F1_D          0.244565  0.333333  0.231660  0.322767  0.036585  0.328889   \n",
       "4_precision_FA  0.905660  0.924759  0.902108  0.916129  0.889670  0.909692   \n",
       "5_recall_FA     0.871350  0.834254  0.945541  0.896606  0.999211  0.977901   \n",
       "6_F1_FA         0.888174  0.877178  0.923314  0.906262  0.941264  0.942564   \n",
       "7_accuracy      0.805186  0.792572  0.860547  0.835319  0.889278  0.894184   \n",
       "\n",
       "                 LEXA  G_NB4s  \n",
       "1_precision_D   0.500   0.493  \n",
       "2_recall_D      0.263   0.206  \n",
       "3_F1_D          0.344   0.291  \n",
       "4_precision_FA  0.913   0.907  \n",
       "5_recall_FA     0.967   0.973  \n",
       "6_F1_FA         0.939   0.939  \n",
       "7_accuracy      0.888   0.888  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for c in classifier:\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, occur,c,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "classifier_test = pd.DataFrame(result+G_test, index=['dtc','mnb','knn','log','svm','voter','LEXA','G_NB4s'])\n",
    "classifier_test.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using occurrence as our features, we saw the training and testing performance of the models trained by different algorithm compare to LEXA and G_NB4s(Naïve Bayes Classifier based on sentences by Galgani). All the models showed high accuracy as expected. MNB and KNN’s F1_measure(D)(F1_D) is low shows that their performance on classifying the minority class was poor. SVM classified the minority class with a high precision but it didn’t capture only a few of the class D citation (low recall). Decision tree and logistic regression shows a good result in training set but they all overfitted the training data and performance dropped significantly in test data. \n",
    "However, by combining all these weaker models with voting, we build a model which performed as well as LEXA. The precision of our ensemble model is better than LEXA but it did not capture as much minority class as LEXA did.\n",
    "\n",
    "### Test of different number of words in a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unigram</th>\n",
       "      <th>2gram</th>\n",
       "      <th>3gram</th>\n",
       "      <th>mixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_precision_D</th>\n",
       "      <td>0.438735</td>\n",
       "      <td>0.411844</td>\n",
       "      <td>0.305315</td>\n",
       "      <td>0.371298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_recall_D</th>\n",
       "      <td>0.723913</td>\n",
       "      <td>0.665217</td>\n",
       "      <td>0.536957</td>\n",
       "      <td>0.708696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_F1_D</th>\n",
       "      <td>0.546349</td>\n",
       "      <td>0.508728</td>\n",
       "      <td>0.389283</td>\n",
       "      <td>0.487294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_precision_FA</th>\n",
       "      <td>0.960201</td>\n",
       "      <td>0.951980</td>\n",
       "      <td>0.932187</td>\n",
       "      <td>0.956380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_recall_FA</th>\n",
       "      <td>0.877937</td>\n",
       "      <td>0.874785</td>\n",
       "      <td>0.838968</td>\n",
       "      <td>0.841834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_F1_FA</th>\n",
       "      <td>0.917228</td>\n",
       "      <td>0.911752</td>\n",
       "      <td>0.883125</td>\n",
       "      <td>0.895459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_accuracy</th>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.850380</td>\n",
       "      <td>0.803797</td>\n",
       "      <td>0.826329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 unigram     2gram     3gram     mixed\n",
       "1_precision_D   0.438735  0.411844  0.305315  0.371298\n",
       "2_recall_D      0.723913  0.665217  0.536957  0.708696\n",
       "3_F1_D          0.546349  0.508728  0.389283  0.487294\n",
       "4_precision_FA  0.960201  0.951980  0.932187  0.956380\n",
       "5_recall_FA     0.877937  0.874785  0.838968  0.841834\n",
       "6_F1_FA         0.917228  0.911752  0.883125  0.895459\n",
       "7_accuracy      0.860000  0.850380  0.803797  0.826329"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram = [occur,occur_2,occur_3,occur_123]\n",
    "result = []\n",
    "for f in gram:\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), train.text, f,mnb,ros)\n",
    "    score = model_score(train.label,predicted)\n",
    "    result.append(score)\n",
    "gram_train = pd.DataFrame(result,index=['unigram','2gram','3gram','mixed'])\n",
    "gram_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unigram</th>\n",
       "      <th>2gram</th>\n",
       "      <th>3gram</th>\n",
       "      <th>mixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_precision_D</th>\n",
       "      <td>0.260563</td>\n",
       "      <td>0.204152</td>\n",
       "      <td>0.168224</td>\n",
       "      <td>0.223214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_recall_D</th>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.368750</td>\n",
       "      <td>0.337500</td>\n",
       "      <td>0.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_F1_D</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.262806</td>\n",
       "      <td>0.224532</td>\n",
       "      <td>0.302419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_precision_FA</th>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.911248</td>\n",
       "      <td>0.904159</td>\n",
       "      <td>0.922090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_recall_FA</th>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.818469</td>\n",
       "      <td>0.789266</td>\n",
       "      <td>0.794002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_F1_FA</th>\n",
       "      <td>0.877178</td>\n",
       "      <td>0.862370</td>\n",
       "      <td>0.842815</td>\n",
       "      <td>0.853265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_accuracy</th>\n",
       "      <td>0.792572</td>\n",
       "      <td>0.768045</td>\n",
       "      <td>0.738612</td>\n",
       "      <td>0.757533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 unigram     2gram     3gram     mixed\n",
       "1_precision_D   0.260563  0.204152  0.168224  0.223214\n",
       "2_recall_D      0.462500  0.368750  0.337500  0.468750\n",
       "3_F1_D          0.333333  0.262806  0.224532  0.302419\n",
       "4_precision_FA  0.924759  0.911248  0.904159  0.922090\n",
       "5_recall_FA     0.834254  0.818469  0.789266  0.794002\n",
       "6_F1_FA         0.877178  0.862370  0.842815  0.853265\n",
       "7_accuracy      0.792572  0.768045  0.738612  0.757533"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram = [occur,occur_2,occur_3,occur_123]\n",
    "result = []\n",
    "for f in gram:\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, f,mnb,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "gram_test =pd.DataFrame(result,index=['unigram','2gram','3gram','mixed'])\n",
    "gram_test.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For number of grams, unigram worked the best and as number of word per token increased, the model has a poorer performance. However, a potential reason for the result can be result from us removing the stop word in data preprocessing which destructed the meaning of multiple words.\n",
    "\n",
    "### Test of different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occur</th>\n",
       "      <th>count</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_precision_D</th>\n",
       "      <td>0.438735</td>\n",
       "      <td>0.369032</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_recall_D</th>\n",
       "      <td>0.723913</td>\n",
       "      <td>0.621739</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_F1_D</th>\n",
       "      <td>0.546349</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.094650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_precision_FA</th>\n",
       "      <td>0.960201</td>\n",
       "      <td>0.945197</td>\n",
       "      <td>0.888634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_recall_FA</th>\n",
       "      <td>0.877937</td>\n",
       "      <td>0.859885</td>\n",
       "      <td>0.999140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_F1_FA</th>\n",
       "      <td>0.917228</td>\n",
       "      <td>0.900525</td>\n",
       "      <td>0.940653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_accuracy</th>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.832152</td>\n",
       "      <td>0.888608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   occur     count     tfidf\n",
       "1_precision_D   0.438735  0.369032  0.884615\n",
       "2_recall_D      0.723913  0.621739  0.050000\n",
       "3_F1_D          0.546349  0.463158  0.094650\n",
       "4_precision_FA  0.960201  0.945197  0.888634\n",
       "5_recall_FA     0.877937  0.859885  0.999140\n",
       "6_F1_FA         0.917228  0.900525  0.940653\n",
       "7_accuracy      0.860000  0.832152  0.888608"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = [occur,count,tfidf]\n",
    "result = []\n",
    "for f in feature:\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), train.text, f,mnb,ros)\n",
    "    score = model_score(train.label,predicted)\n",
    "    result.append(score)\n",
    "feature_train = pd.DataFrame(result,index=['occur','count','tfidf'])\n",
    "feature_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/garylau/Documents/Macquarie University/LocalStudy/ITEC873/legal_citation_classification/model_utility.py:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  F1_D = 2*(precision_D*recall_D)/(precision_D+recall_D)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occur</th>\n",
       "      <th>count</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_precision_D</th>\n",
       "      <td>0.260563</td>\n",
       "      <td>0.232082</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_recall_D</th>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_F1_D</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300221</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_precision_FA</th>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.918871</td>\n",
       "      <td>0.887798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_recall_FA</th>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.822415</td>\n",
       "      <td>0.999211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_F1_FA</th>\n",
       "      <td>0.877178</td>\n",
       "      <td>0.867972</td>\n",
       "      <td>0.940215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_accuracy</th>\n",
       "      <td>0.792572</td>\n",
       "      <td>0.777856</td>\n",
       "      <td>0.887176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   occur     count     tfidf\n",
       "1_precision_D   0.260563  0.232082  0.000000\n",
       "2_recall_D      0.462500  0.425000  0.000000\n",
       "3_F1_D          0.333333  0.300221       NaN\n",
       "4_precision_FA  0.924759  0.918871  0.887798\n",
       "5_recall_FA     0.834254  0.822415  0.999211\n",
       "6_F1_FA         0.877178  0.867972  0.940215\n",
       "7_accuracy      0.792572  0.777856  0.887176"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = [occur,count,tfidf]\n",
    "result = []\n",
    "for f in feature:\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, f,mnb,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "feature_test = pd.DataFrame(result,index=['occur','count','tfidf'])\n",
    "feature_test.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MNB as our algorithm, we set up a test on different type of features. Although tfidf performed the best in the training stage, it showed a low recall(D) which means the model cannot capture the minority class despite having a high precision. As result, occurrence of a particular word is a better feature to use. This also shows that the occurrence of a particular word is more important than how many times the word appeared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "To deal with the imbalance of the data set, we adopted  naïve random oversampling to solve this issues. However, oversampling did not provide more information about the minority class. This leads to all the models failed to capture class D citations. In future study, we can explore under-sampling technique to combine majority class observations so the features that both class shared can be less overlap. \n",
    "\n",
    "\n",
    "As applying ensemble technique shows some performance improvement, future study can be focus on adjusting the weight of the weaker models and even exploring other ensemble method such as boosting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "All the Grid Search result for tuning the parameter of the model will be store in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_D</th>\n",
       "      <th>F1_FA</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_D</th>\n",
       "      <th>precision_FA</th>\n",
       "      <th>recall_D</th>\n",
       "      <th>recall_FA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.898949</td>\n",
       "      <td>0.824807</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.921292</td>\n",
       "      <td>0.40625</td>\n",
       "      <td>0.877664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.900405</td>\n",
       "      <td>0.827610</td>\n",
       "      <td>0.308036</td>\n",
       "      <td>0.924356</td>\n",
       "      <td>0.43125</td>\n",
       "      <td>0.877664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.879339</td>\n",
       "      <td>0.795375</td>\n",
       "      <td>0.259124</td>\n",
       "      <td>0.922810</td>\n",
       "      <td>0.44375</td>\n",
       "      <td>0.839779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.857748</td>\n",
       "      <td>0.764541</td>\n",
       "      <td>0.234940</td>\n",
       "      <td>0.925114</td>\n",
       "      <td>0.48750</td>\n",
       "      <td>0.799526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.853617</td>\n",
       "      <td>0.758935</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.926131</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.791634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.313576</td>\n",
       "      <td>0.845989</td>\n",
       "      <td>0.748423</td>\n",
       "      <td>0.225895</td>\n",
       "      <td>0.926692</td>\n",
       "      <td>0.51250</td>\n",
       "      <td>0.778216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.841832</td>\n",
       "      <td>0.743518</td>\n",
       "      <td>0.228947</td>\n",
       "      <td>0.930277</td>\n",
       "      <td>0.54375</td>\n",
       "      <td>0.768745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.835366</td>\n",
       "      <td>0.735109</td>\n",
       "      <td>0.226131</td>\n",
       "      <td>0.931973</td>\n",
       "      <td>0.56250</td>\n",
       "      <td>0.756906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.316163</td>\n",
       "      <td>0.831951</td>\n",
       "      <td>0.730203</td>\n",
       "      <td>0.220844</td>\n",
       "      <td>0.930664</td>\n",
       "      <td>0.55625</td>\n",
       "      <td>0.752170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         F1_D     F1_FA  accuracy  precision_D  precision_FA  recall_D  \\\n",
       "0.2  0.342105  0.898949  0.824807     0.295455      0.921292   0.40625   \n",
       "0.3  0.359375  0.900405  0.827610     0.308036      0.924356   0.43125   \n",
       "0.4  0.327189  0.879339  0.795375     0.259124      0.922810   0.44375   \n",
       "0.5  0.317073  0.857748  0.764541     0.234940      0.925114   0.48750   \n",
       "0.6  0.317460  0.853617  0.758935     0.232558      0.926131   0.50000   \n",
       "0.7  0.313576  0.845989  0.748423     0.225895      0.926692   0.51250   \n",
       "0.8  0.322222  0.841832  0.743518     0.228947      0.930277   0.54375   \n",
       "0.9  0.322581  0.835366  0.735109     0.226131      0.931973   0.56250   \n",
       "1.0  0.316163  0.831951  0.730203     0.220844      0.930664   0.55625   \n",
       "\n",
       "     recall_FA  \n",
       "0.2   0.877664  \n",
       "0.3   0.877664  \n",
       "0.4   0.839779  \n",
       "0.5   0.799526  \n",
       "0.6   0.791634  \n",
       "0.7   0.778216  \n",
       "0.8   0.768745  \n",
       "0.9   0.756906  \n",
       "1.0   0.752170  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio for oversampling\n",
    "ratio = [i/10 for i in range(2,11)]\n",
    "result = []\n",
    "for r in ratio:\n",
    "    ros = RandomOverSampler(random_state=0, sampling_strategy = r)\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, occur,mnb,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "pd.DataFrame(result,index=ratio).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>0.7</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_D</th>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.329571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_FA</th>\n",
       "      <td>0.877280</td>\n",
       "      <td>0.876815</td>\n",
       "      <td>0.876815</td>\n",
       "      <td>0.876815</td>\n",
       "      <td>0.876815</td>\n",
       "      <td>0.876815</td>\n",
       "      <td>0.877178</td>\n",
       "      <td>0.877178</td>\n",
       "      <td>0.877178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.792572</td>\n",
       "      <td>0.791871</td>\n",
       "      <td>0.791871</td>\n",
       "      <td>0.791871</td>\n",
       "      <td>0.791871</td>\n",
       "      <td>0.791871</td>\n",
       "      <td>0.792572</td>\n",
       "      <td>0.792572</td>\n",
       "      <td>0.792572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_D</th>\n",
       "      <td>0.258865</td>\n",
       "      <td>0.257951</td>\n",
       "      <td>0.257951</td>\n",
       "      <td>0.257951</td>\n",
       "      <td>0.257951</td>\n",
       "      <td>0.257951</td>\n",
       "      <td>0.260563</td>\n",
       "      <td>0.260563</td>\n",
       "      <td>0.260563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_FA</th>\n",
       "      <td>0.924017</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.923951</td>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.924759</td>\n",
       "      <td>0.924759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_D</th>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.456250</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_FA</th>\n",
       "      <td>0.835043</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "      <td>0.834254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0.1       0.2       0.3       0.4       0.5       0.6  \\\n",
       "F1_D          0.330317  0.329571  0.329571  0.329571  0.329571  0.329571   \n",
       "F1_FA         0.877280  0.876815  0.876815  0.876815  0.876815  0.876815   \n",
       "accuracy      0.792572  0.791871  0.791871  0.791871  0.791871  0.791871   \n",
       "precision_D   0.258865  0.257951  0.257951  0.257951  0.257951  0.257951   \n",
       "precision_FA  0.924017  0.923951  0.923951  0.923951  0.923951  0.923951   \n",
       "recall_D      0.456250  0.456250  0.456250  0.456250  0.456250  0.456250   \n",
       "recall_FA     0.835043  0.834254  0.834254  0.834254  0.834254  0.834254   \n",
       "\n",
       "                   0.7       0.8       0.9  \n",
       "F1_D          0.333333  0.333333  0.333333  \n",
       "F1_FA         0.877178  0.877178  0.877178  \n",
       "accuracy      0.792572  0.792572  0.792572  \n",
       "precision_D   0.260563  0.260563  0.260563  \n",
       "precision_FA  0.924759  0.924759  0.924759  \n",
       "recall_D      0.462500  0.462500  0.462500  \n",
       "recall_FA     0.834254  0.834254  0.834254  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check alpha for multinormial naive bayes\n",
    "ros = RandomOverSampler(random_state=0, sampling_strategy = 0.3)\n",
    "alpha = [i/10 for i in range(1,10)]\n",
    "result = []\n",
    "for r in alpha:\n",
    "#     ros = RandomOverSampler(random_state=0, sampling_strategy = r)\n",
    "    mnb = MultinomialNB(alpha=r)\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, occur,mnb,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "CV = pd.DataFrame(result,index=alpha)\n",
    "CV.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_D</th>\n",
       "      <td>0.250681</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.245014</td>\n",
       "      <td>0.257534</td>\n",
       "      <td>0.219718</td>\n",
       "      <td>0.225352</td>\n",
       "      <td>0.205714</td>\n",
       "      <td>0.206186</td>\n",
       "      <td>0.247253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_FA</th>\n",
       "      <td>0.889425</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>0.899682</td>\n",
       "      <td>0.894127</td>\n",
       "      <td>0.891121</td>\n",
       "      <td>0.889156</td>\n",
       "      <td>0.889956</td>\n",
       "      <td>0.888978</td>\n",
       "      <td>0.875101</td>\n",
       "      <td>0.889960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807288</td>\n",
       "      <td>0.805186</td>\n",
       "      <td>0.823406</td>\n",
       "      <td>0.814296</td>\n",
       "      <td>0.810091</td>\n",
       "      <td>0.805886</td>\n",
       "      <td>0.807288</td>\n",
       "      <td>0.805186</td>\n",
       "      <td>0.784163</td>\n",
       "      <td>0.807989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_D</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.229358</td>\n",
       "      <td>0.247253</td>\n",
       "      <td>0.225131</td>\n",
       "      <td>0.229268</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.220588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_FA</th>\n",
       "      <td>0.906557</td>\n",
       "      <td>0.909016</td>\n",
       "      <td>0.907631</td>\n",
       "      <td>0.905340</td>\n",
       "      <td>0.907529</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.902597</td>\n",
       "      <td>0.899757</td>\n",
       "      <td>0.899917</td>\n",
       "      <td>0.905969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_D</th>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>0.293750</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_FA</th>\n",
       "      <td>0.872928</td>\n",
       "      <td>0.867403</td>\n",
       "      <td>0.891871</td>\n",
       "      <td>0.883189</td>\n",
       "      <td>0.875296</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>0.877664</td>\n",
       "      <td>0.878453</td>\n",
       "      <td>0.851618</td>\n",
       "      <td>0.874507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    10        11        12        13        14        15  \\\n",
       "F1_D          0.250681  0.264550  0.263158  0.245014  0.257534  0.219718   \n",
       "F1_FA         0.889425  0.887722  0.899682  0.894127  0.891121  0.889156   \n",
       "accuracy      0.807288  0.805186  0.823406  0.814296  0.810091  0.805886   \n",
       "precision_D   0.222222  0.229358  0.247253  0.225131  0.229268  0.200000   \n",
       "precision_FA  0.906557  0.909016  0.907631  0.905340  0.907529  0.901786   \n",
       "recall_D      0.287500  0.312500  0.281250  0.268750  0.293750  0.243750   \n",
       "recall_FA     0.872928  0.867403  0.891871  0.883189  0.875296  0.876875   \n",
       "\n",
       "                    16        17        18        19  \n",
       "F1_D          0.225352  0.205714  0.206186  0.247253  \n",
       "F1_FA         0.889956  0.888978  0.875101  0.889960  \n",
       "accuracy      0.807288  0.805186  0.784163  0.807989  \n",
       "precision_D   0.205128  0.189474  0.175439  0.220588  \n",
       "precision_FA  0.902597  0.899757  0.899917  0.905969  \n",
       "recall_D      0.250000  0.225000  0.250000  0.281250  \n",
       "recall_FA     0.877664  0.878453  0.851618  0.874507  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check decision tree best depth\n",
    "depth = [i for i in range(10,20,1)]\n",
    "result = []\n",
    "for d in depth:\n",
    "#     ros = RandomOverSampler(random_state=0, sampling_strategy = r)\n",
    "    dtc = tree.DecisionTreeClassifier(max_depth=d)\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, occur,dtc,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "CV = pd.DataFrame(result,index=depth)\n",
    "CV.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1_D</th>\n",
       "      <td>0.182432</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.211604</td>\n",
       "      <td>0.217105</td>\n",
       "      <td>0.208633</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.229630</td>\n",
       "      <td>0.233216</td>\n",
       "      <td>0.231660</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.229508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_FA</th>\n",
       "      <td>0.905395</td>\n",
       "      <td>0.900197</td>\n",
       "      <td>0.909801</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.914596</td>\n",
       "      <td>0.913349</td>\n",
       "      <td>0.919505</td>\n",
       "      <td>0.915597</td>\n",
       "      <td>0.923314</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.927969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.830413</td>\n",
       "      <td>0.822705</td>\n",
       "      <td>0.838122</td>\n",
       "      <td>0.833217</td>\n",
       "      <td>0.845830</td>\n",
       "      <td>0.844429</td>\n",
       "      <td>0.854240</td>\n",
       "      <td>0.847933</td>\n",
       "      <td>0.860547</td>\n",
       "      <td>0.857043</td>\n",
       "      <td>0.868255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_D</th>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.233083</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.265152</td>\n",
       "      <td>0.281818</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision_FA</th>\n",
       "      <td>0.896979</td>\n",
       "      <td>0.899842</td>\n",
       "      <td>0.900309</td>\n",
       "      <td>0.901013</td>\n",
       "      <td>0.899924</td>\n",
       "      <td>0.903475</td>\n",
       "      <td>0.902050</td>\n",
       "      <td>0.902607</td>\n",
       "      <td>0.902108</td>\n",
       "      <td>0.903569</td>\n",
       "      <td>0.901713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_D</th>\n",
       "      <td>0.168750</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.181250</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.193750</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall_FA</th>\n",
       "      <td>0.913970</td>\n",
       "      <td>0.900552</td>\n",
       "      <td>0.919495</td>\n",
       "      <td>0.912391</td>\n",
       "      <td>0.929755</td>\n",
       "      <td>0.923441</td>\n",
       "      <td>0.937648</td>\n",
       "      <td>0.928966</td>\n",
       "      <td>0.945541</td>\n",
       "      <td>0.939227</td>\n",
       "      <td>0.955801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    19        20        21        22        23        24  \\\n",
       "F1_D          0.182432  0.206897  0.211604  0.217105  0.208633  0.239726   \n",
       "F1_FA         0.905395  0.900197  0.909801  0.906667  0.914596  0.913349   \n",
       "accuracy      0.830413  0.822705  0.838122  0.833217  0.845830  0.844429   \n",
       "precision_D   0.198529  0.207547  0.233083  0.229167  0.245763  0.265152   \n",
       "precision_FA  0.896979  0.899842  0.900309  0.901013  0.899924  0.903475   \n",
       "recall_D      0.168750  0.206250  0.193750  0.206250  0.181250  0.218750   \n",
       "recall_FA     0.913970  0.900552  0.919495  0.912391  0.929755  0.923441   \n",
       "\n",
       "                    25        26        27        28        29  \n",
       "F1_D          0.229630  0.233216  0.231660  0.244444  0.229508  \n",
       "F1_FA         0.919505  0.915597  0.923314  0.921053  0.927969  \n",
       "accuracy      0.854240  0.847933  0.860547  0.857043  0.868255  \n",
       "precision_D   0.281818  0.268293  0.303030  0.300000  0.333333  \n",
       "precision_FA  0.902050  0.902607  0.902108  0.903569  0.901713  \n",
       "recall_D      0.193750  0.206250  0.187500  0.206250  0.175000  \n",
       "recall_FA     0.937648  0.928966  0.945541  0.939227  0.955801  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of neighbor for knn\n",
    "neigh = [i for i in range(20,31,1)]\n",
    "result = []\n",
    "for k in neigh:\n",
    "#     ros = RandomOverSampler(random_state=0, sampling_strategy = r)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    predicted = nlp_pipeline(train.text, train.label.ravel(), test.text, occur,knn,ros)\n",
    "    score = model_score(test.label,predicted)\n",
    "    result.append(score)\n",
    "CV = pd.DataFrame(result,index=neigh)\n",
    "CV.transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
